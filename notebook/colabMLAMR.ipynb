{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# v0.0.1 of colab-ML-AMR\n",
        "\n",
        "Select species, bin_size, models and dataset.\n",
        "\n",
        "Run all -> Choose files (zip file of MALDI-TOF MS spectra)"
      ],
      "metadata": {
        "id": "anZf0Omp8uOc"
      },
      "id": "anZf0Omp8uOc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "520ba867-c565-453b-91f7-282fe4f774d2",
      "metadata": {
        "cellView": "form",
        "id": "520ba867-c565-453b-91f7-282fe4f774d2"
      },
      "outputs": [],
      "source": [
        "#@title Input protein sequence(s), then hit `Runtime` -> `Run all`\n",
        "from google.colab import files\n",
        "import os\n",
        "import re\n",
        "import hashlib\n",
        "import random\n",
        "import glob\n",
        "\n",
        "from sys import version_info\n",
        "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
        "\n",
        "def add_hash(x,y):\n",
        "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "species = \"Staphylococcus_aureus\" #@param ['Escherichia_coli', 'Klebsiella_pneumoniae','Staphylococcus_aureus'] {type:\"string\"}\n",
        "#@markdown  - Use `_` to concat genus_species name\n",
        "bin_size = 3 #@param [3] {type:\"raw\"}\n",
        "#@markdown - (Da) 3 means that 6,000 bins of 2,000~20,000 Da of spectra range\n",
        "models = 'All' #@param ['All', 'LogReg'] {type:\"raw\"}\n",
        "#@markdown - specify pretrained models\n",
        "dataset = 'UMG' #@param ['UMG'] {type:\"raw\"}\n",
        "#@markdown - specify pretrained models\n",
        "#jobname = 'test' #@param {type:\"string\"}\n",
        "\n",
        "\"\"\"\n",
        "# remove whitespaces\n",
        "#basejobname = \"\".join(jobname.split())\n",
        "#basejobname = re.sub(r'\\W+', '', basejobname)\n",
        "#jobname = add_hash(basejobname, species)\n",
        "\n",
        "# check if directory with jobname exists\n",
        "def check(folder):\n",
        "  if os.path.exists(folder):\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "if not check(jobname):\n",
        "  n = 0\n",
        "  while not check(f\"{jobname}_{n}\"): n += 1\n",
        "  jobname = f\"{jobname}_{n}\"\n",
        "\"\"\"\n",
        "\n",
        "!rm -rf ./data/* \n",
        "# make directory to save results\n",
        "data_store_path = os.path.join('data', 'raw', species)\n",
        "os.makedirs(data_store_path, exist_ok=True)\n",
        "\n",
        "# Number of bins calculate\n",
        "number_of_bins = int(round(18000/bin_size))\n",
        "\n",
        "uploaded = files.upload()\n",
        "use_templates = True\n",
        "for fn in uploaded.keys():\n",
        "  new_fname = os.path.join(data_store_path,fn)\n",
        "  os.rename(fn, new_fname)\n",
        "  if '.zip' in fn:\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(new_fname, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_store_path)\n",
        "    os.remove(new_fname)\n",
        "  elif '.tar.gz' in fn:\n",
        "    import tarfile\n",
        "    if new_fname.endswith(\"tar.gz\"):\n",
        "        tar = tarfile.open(new_fname, \"r:gz\")\n",
        "        tar.extractall()\n",
        "        tar.close()\n",
        "    elif new_fname.endswith(\"tar\"):\n",
        "        tar = tarfile.open(new_fname, \"r:\")\n",
        "        tar.extractall()\n",
        "        tar.close()\n",
        "    os.remove(new_fname)\n",
        "\n",
        "# Dataset filter\n",
        "dataset_convert = {\n",
        "    \"All\": '*',\n",
        "    \"UMG\": 'UMG-0',\n",
        "    \"DRIAMS-A\": 'DRIAMS-A',\n",
        "}\n",
        "dataset_search = dataset_convert[dataset]\n",
        "\n",
        "# Model filter\n",
        "model_convert = {\n",
        "    \"All\": '*',\n",
        "    \"LogReg\": 'lr',\n",
        "    \"LightGBM\": 'LightGBM',\n",
        "    \"MLP\": 'MLP',\n",
        "}\n",
        "model_search = model_convert[models]\n",
        "\n",
        "# model download\n",
        "model_store_path = os.path.join('models', 'sklearn')\n",
        "os.makedirs(model_store_path, exist_ok=True)\n",
        "\n",
        "model_download_check = os.path.join(model_store_path, species, f'Train_site_{dataset_search}_Model_{model_search}_Species_{species}*')\n",
        "if len(glob.glob(model_download_check)) == 0:\n",
        "  !gdown 1Atbf3y7xM58ZIbRXVAnvDPCT2aR_t_tJ\n",
        "\n",
        "  # unzip model\n",
        "  import glob\n",
        "  model_file = 'sklearn_13092024.zip'\n",
        "  nfilename = os.path.join(model_store_path, model_file)\n",
        "  os.rename(model_file, nfilename)\n",
        "  with zipfile.ZipFile(nfilename, 'r') as zip_ref:\n",
        "      zip_ref.extractall(model_store_path)\n",
        "else:\n",
        "  print('models are exist!')\n",
        "#\n",
        "print(\"dataset\", dataset_search)\n",
        "print(\"model\", model_search)\n",
        "print(\"species\",species)\n",
        "print(\"bin size:\",bin_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "834264ac-d9af-4784-b348-11fe90ded44a",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "834264ac-d9af-4784-b348-11fe90ded44a"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100fbc6d-0029-4535-9dfe-b5b659a7bc68",
      "metadata": {
        "id": "100fbc6d-0029-4535-9dfe-b5b659a7bc68"
      },
      "outputs": [],
      "source": [
        "!python -m pip install rpy2==3.5 pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc19b70-2623-43f6-88de-b14109032662",
      "metadata": {
        "id": "8bc19b70-2623-43f6-88de-b14109032662"
      },
      "outputs": [],
      "source": [
        "# import rpy2's package module\n",
        "import rpy2.robjects.packages as rpackages\n",
        "\n",
        "# import R's utility package\n",
        "utils = rpackages.importr('utils')\n",
        "\n",
        "# select a mirror for R packages\n",
        "utils.chooseCRANmirror(ind=1) # select the first mirror in the list\n",
        "\n",
        "# R package names\n",
        "packnames = ('readBrukerFlexData', 'MALDIquant')\n",
        "\n",
        "# R vector of strings\n",
        "from rpy2.robjects.vectors import StrVector\n",
        "\n",
        "# Selectively install what needs to be install.\n",
        "# We are fancy, just because we can.\n",
        "names_to_install = [x for x in packnames if not rpackages.isinstalled(x)]\n",
        "if len(names_to_install) > 0:\n",
        "    utils.install_packages(StrVector(names_to_install))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6615616c-37f7-4a89-be4d-ce153b4d4df4",
      "metadata": {
        "id": "6615616c-37f7-4a89-be4d-ce153b4d4df4"
      },
      "source": [
        "### Spectra preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ab8e329-3361-4ada-9316-431dcc974384",
      "metadata": {
        "id": "6ab8e329-3361-4ada-9316-431dcc974384"
      },
      "outputs": [],
      "source": [
        "import rpy2.robjects as robjects\n",
        "\n",
        "robjects.r('''\n",
        "require(\"readBrukerFlexData\")\n",
        "require(\"MALDIquant\")\n",
        "readBruker <- function(path_, out_path_) {\n",
        "    sample <- readBrukerFlexDir(path_, removeCalibrationScans = TRUE,\n",
        "                                removeMetaData = FALSE, useHpc = TRUE, useSpectraNames = TRUE,\n",
        "                                filterZeroIntensities = FALSE, verbose = FALSE)\n",
        "\n",
        "    m <- sample[[1]][[1]]$mass\n",
        "    i <- sample[[1]][[1]]$intensity\n",
        "\n",
        "    #Basic Preprocessing\n",
        "    # (In this version the data was NOT transformed and normalized. That will hapen later in python)\n",
        "    spectra <- createMassSpectrum(mass = m, intensity = i)\n",
        "    spectra <- transformIntensity(spectra, method=\"sqrt\")\n",
        "    spectra <- smoothIntensity(spectra, method=\"SavitzkyGolay\", halfWindowSize=10)\n",
        "    spectra <- removeBaseline(spectra, method=\"SNIP\", iterations=20)\n",
        "    spectra <- calibrateIntensity(spectra, method=\"TIC\")\n",
        "    spectra <- trim(spectra, range=c(2000, 20000))\n",
        "\n",
        "    #pks <- detectPeaks(spectra, method=\"MAD\", halfWindowSize=20, SNR=4)\n",
        "    pks <- spectra\n",
        "\n",
        "    mass <- mass(pks)\n",
        "    intensity <- intensity(pks)\n",
        "    small_dataframe <- data.frame(mass, intensity, stringsAsFactors = FALSE)\n",
        "\n",
        "    write.table(small_dataframe, out_path_, row.names = F, col.names = F)\n",
        "    }\n",
        "    ''')\n",
        "read_bruker = robjects.globalenv['readBruker']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b451ea12-8c1d-45a9-b901-5ecbcd0b8d3e",
      "metadata": {
        "id": "b451ea12-8c1d-45a9-b901-5ecbcd0b8d3e"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_columns', None)  # or 1000\n",
        "pd.set_option('display.max_rows', None)  # or 1000\n",
        "pd.set_option('display.max_colwidth', None)  # or 199\n",
        "\n",
        "def folder_scan(raw_dir: str) -> dict:\n",
        "    file_exist_dic = {}\n",
        "    raw_file_path = os.path.join(raw_dir, '*', '*')\n",
        "    raw_file_list = glob.glob(raw_file_path)\n",
        "    for filepath in raw_file_list:\n",
        "        species_name, sample_number = filepath.split(os.sep)[-2:]\n",
        "        if species_name not in file_exist_dic.keys():\n",
        "            file_exist_dic[species_name] = set()\n",
        "        file_exist_dic[species_name].add(sample_number)\n",
        "    print(f'File scan done.')\n",
        "\n",
        "    return file_exist_dic\n",
        "\n",
        "\n",
        "def preprocessing(raw_dir: str, preprocessed_dir: str, file_exist_dic: dict) -> None:\n",
        "    for species in file_exist_dic.keys():\n",
        "        raw_path_species = os.path.join(raw_dir, species)\n",
        "        preprocessed_path = os.path.join(preprocessed_dir, species)\n",
        "        os.makedirs(preprocessed_path, exist_ok=True)\n",
        "        for sample_number in file_exist_dic[species]:\n",
        "            raw_path = os.path.join(raw_path_species, sample_number)\n",
        "            preprocessed_filepath = os.path.join(preprocessed_path, sample_number)\n",
        "            preprocessed_filepath = f'{preprocessed_filepath}.txt'\n",
        "\n",
        "            if os.path.exists(preprocessed_filepath):\n",
        "                #print(f'Preprocessing {preprocessed_filepath} already exist.')\n",
        "                continue\n",
        "\n",
        "            print(f'New raw file: {raw_path} found.')\n",
        "            read_bruker(raw_path, preprocessed_filepath)\n",
        "\n",
        "            try:\n",
        "                read_bruker(raw_path, preprocessed_filepath)\n",
        "                print(f'Preprocessing {preprocessed_filepath} done.')\n",
        "            except:\n",
        "                print(f'Preprocessing of {raw_path} fail.')\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def bin_vectorize(preprocessed_file: str, binned_file: str, bin_size: int) -> None:\n",
        "    spectra = pd.read_csv(preprocessed_file, sep=' ', index_col=False, header=None).to_numpy()\n",
        "    combined_times = spectra[:, 0]\n",
        "    min_range = min(2000, np.min(combined_times))\n",
        "    max_range = max(20000, np.max(combined_times))\n",
        "\n",
        "    _, bin_edges_ = np.histogram(combined_times, bin_size, range=(min_range, max_range))\n",
        "\n",
        "    times = spectra[:, 0]\n",
        "    indices = np.digitize(times, bin_edges_, right=True)\n",
        "\n",
        "    valid = (indices >= 1) & (indices <= bin_size)\n",
        "    spectrum = spectra[valid]\n",
        "\n",
        "    # Need to update indices to ensure that the first bin is at\n",
        "    # position zero.\n",
        "    indices = indices[valid] - 1\n",
        "    identity = np.eye(bin_size)\n",
        "\n",
        "    vec = np.sum(identity[indices] * spectra[:, 1][:, np.newaxis], axis=0)\n",
        "    np.savetxt(binned_file, vec, delimiter=\",\")\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def binning(preprocessed_dir: str, binned_dir: str, file_todo_dic: dict, bin_size: int) -> None:\n",
        "    for species in file_todo_dic.keys():\n",
        "        preprocessed_path_species = os.path.join(preprocessed_dir, species)\n",
        "        binned_path = os.path.join(binned_dir, species)\n",
        "        os.makedirs(binned_path, exist_ok=True)\n",
        "        for sample_number in file_todo_dic[species]:\n",
        "            preprocessed_file_path = os.path.join(preprocessed_path_species, sample_number)\n",
        "            sample_outpath = os.path.join(binned_path, sample_number)\n",
        "\n",
        "            if os.path.exists(sample_outpath):\n",
        "                print(f'Preprocessing {sample_outpath} already exist.')\n",
        "                continue\n",
        "\n",
        "            print(f'New preprocessed file: {preprocessed_file_path} found.')\n",
        "            bin_vectorize(preprocessed_file_path, sample_outpath, bin_size)\n",
        "\n",
        "            try:\n",
        "                bin_vectorize(preprocessed_file_path, sample_outpath, bin_size)\n",
        "                print(f'Binning {sample_outpath} done.')\n",
        "            except:\n",
        "                print(f'Binning of {preprocessed_file_path} fail.')\n",
        "\n",
        "\n",
        "\n",
        "def scan_preprocessing(bin_size: int) -> None:\n",
        "    raw_dir = os.path.join('.', 'data', 'raw')\n",
        "    preprocessed_dir = os.path.join('.', 'data', 'preprocessed')\n",
        "    binned_dir = os.path.join('.', 'data', f'binned_{str(bin_size)}')\n",
        "    os.makedirs(raw_dir, exist_ok=True)\n",
        "    os.makedirs(preprocessed_dir, exist_ok=True)\n",
        "    os.makedirs(binned_dir, exist_ok=True)\n",
        "\n",
        "    file_exist_dic = folder_scan(raw_dir)\n",
        "    print(file_exist_dic)\n",
        "    preprocessing(raw_dir, preprocessed_dir, file_exist_dic)\n",
        "\n",
        "    preprocessed_exist_dic = folder_scan(preprocessed_dir)\n",
        "    print(preprocessed_exist_dic)\n",
        "    _ = binning(preprocessed_dir, binned_dir, preprocessed_exist_dic, bin_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa89efa-cd59-4665-92a2-101323a6e5bd",
      "metadata": {
        "id": "6fa89efa-cd59-4665-92a2-101323a6e5bd"
      },
      "outputs": [],
      "source": [
        "scan_preprocessing(number_of_bins)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "920c48a8-8fef-451b-95af-bf5d741d2c1d",
      "metadata": {
        "id": "920c48a8-8fef-451b-95af-bf5d741d2c1d"
      },
      "source": [
        "### ML model run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7301350-650d-4bbe-8c8a-ac50c362dbd1",
      "metadata": {
        "id": "a7301350-650d-4bbe-8c8a-ac50c362dbd1"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "model_dir = os.path.join('models','sklearn')\n",
        "\n",
        "all_models = glob.glob(f'{model_dir}/{species}/*{dataset_search}*{model_search}*.joblib')\n",
        "print(all_models)\n",
        "model_dic = []\n",
        "for x in all_models:\n",
        "    to = os.path.basename(x).split('_')\n",
        "    meta_dic = {\n",
        "        'train_site': [to[2]],\n",
        "        'ML': [to[7]],\n",
        "        'species': ['_'.join(to[9:10])],\n",
        "        'antibiotics': [to[12]],\n",
        "        'seed': [to[14]],\n",
        "        'model': joblib.load(x)\n",
        "    }\n",
        "\n",
        "    model_dic.append(meta_dic)\n",
        "len(model_dic)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ML_model_run(model_dic: dict, bin_filepath: str, species: str) -> pd.DataFrame:\n",
        "    vec = pd.read_csv(bin_filepath, sep=' ', index_col=False, header=None).to_numpy()\n",
        "    vec = vec.T\n",
        "\n",
        "    result = pd.DataFrame()\n",
        "    list_of_models = model_dic\n",
        "    for model_item in list_of_models:\n",
        "        model = model_item['model']\n",
        "        meta_data = model_item.copy()\n",
        "        del meta_data['model']\n",
        "        result_row = pd.DataFrame.from_dict(meta_data)\n",
        "        pred = model.predict_proba(vec)[0]\n",
        "        result_row['S'] = pred[0]\n",
        "        result_row['R'] = pred[1]\n",
        "        result = pd.concat([result, result_row], axis = 0)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def folder_scan(raw_dir: str) -> dict:\n",
        "    file_exist_dic = {}\n",
        "    raw_file_path = os.path.join(raw_dir, '*', '*')\n",
        "    raw_file_list = glob.glob(raw_file_path)\n",
        "    for filepath in raw_file_list:\n",
        "        species_name, sample_number = filepath.split(os.sep)[-2:]\n",
        "        if species_name not in file_exist_dic.keys():\n",
        "            file_exist_dic[species_name] = set()\n",
        "        file_exist_dic[species_name].add(sample_number)\n",
        "    print(f'File scan done.')\n",
        "\n",
        "    return file_exist_dic\n",
        "\n",
        "\n",
        "def preprocessing(input_dir: str, output_dir: str, file_exist_dic: dict) -> None:\n",
        "    pred_res = pd.DataFrame()\n",
        "    for species in file_exist_dic.keys():\n",
        "        raw_path_species = os.path.join(input_dir, species)\n",
        "        preprocessed_path = os.path.join(output_dir, species)\n",
        "        os.makedirs(preprocessed_path, exist_ok=True)\n",
        "        for sample_number in file_exist_dic[species]:\n",
        "            raw_path = os.path.join(raw_path_species, sample_number)\n",
        "            preprocessed_filepath = os.path.join(preprocessed_path, sample_number)\n",
        "\n",
        "            if os.path.exists(preprocessed_filepath):\n",
        "                #print(f'Preprocessing {preprocessed_filepath} already exist.')\n",
        "                continue\n",
        "\n",
        "            print(f'New bin file: {raw_path} found.')\n",
        "            pred_res = ML_model_run(model_dic, raw_path, species)\n",
        "\n",
        "            try:\n",
        "                pred_res = ML_model_run(model_dic, raw_path, species)\n",
        "                print(f'ML prediction {preprocessed_filepath} done.')\n",
        "            except:\n",
        "                print(f'ML prediction of {raw_path} fail.')\n",
        "\n",
        "    pred_res.sort_values('antibiotics', inplace=True)\n",
        "    out_dic = []\n",
        "    summary_res = pred_res.groupby('antibiotics')['S'].apply(list)\n",
        "    for i_ in range(len(summary_res)):\n",
        "        row = summary_res.values[i_]\n",
        "        amname = summary_res.index[i_]\n",
        "        row = ['S' if x > 0.5 else 'R' for x in row]\n",
        "        out_dic.append({\n",
        "            'Antibiotics': amname,\n",
        "            'Resistant' : row.count('R'),\n",
        "            'Susceptible': row.count('S')\n",
        "        })\n",
        "        #print(f\"{name} S: {row.count('S')}, R: {row.count('R')}\")\n",
        "\n",
        "    result_df = pd.DataFrame.from_dict(out_dic)\n",
        "    result_df.set_index(result_df['Antibiotics'], inplace=True)\n",
        "    del(result_df['Antibiotics'])\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "binned_dir = os.path.join('.', 'data', f'binned_{str(number_of_bins)}')\n",
        "bin_files = folder_scan(binned_dir)\n",
        "results_dir = os.path.join('.', 'results')\n",
        "\n",
        "\n",
        "final_result = preprocessing(binned_dir, results_dir, bin_files)"
      ],
      "metadata": {
        "id": "__VbkSm9Qb7F"
      },
      "id": "__VbkSm9Qb7F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output"
      ],
      "metadata": {
        "id": "-Zi-Rq1IVuuv"
      },
      "id": "-Zi-Rq1IVuuv"
    },
    {
      "cell_type": "code",
      "source": [
        "final_result"
      ],
      "metadata": {
        "id": "W6WVB0m5QnLz"
      },
      "id": "W6WVB0m5QnLz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "834264ac-d9af-4784-b348-11fe90ded44a",
        "6615616c-37f7-4a89-be4d-ce153b4d4df4",
        "920c48a8-8fef-451b-95af-bf5d741d2c1d"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
